{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is by no means a comprehensive example of all the torch_geometric(PyG) can do, but it illustrates some of it's capabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brown\\miniconda3\\envs\\mainenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data# this contains our dataset\n",
    "import numpy as np\n",
    "from torch_geometric.datasets import Planetoid# this helps us manipulate the dataset\n",
    "from torch_geometric.nn import GCNConv# this is a simple implementation of the GCN convolutional layer\n",
    "import torch_geometric.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):# this is the torch geometric implementation of our GCN model like before, it\n",
    "    # is a lot simpler to implement and way customizeable\n",
    "    def __init__(self, in_feat, hid_feat, out_feat):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_feat, hid_feat)\n",
    "        self.conv2 = GCNConv(hid_feat, out_feat)\n",
    "        self.activation = nn.ReLU()\n",
    "        #self.dropout = nn.Dropout(p=.4)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = self.activation(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training= self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([T.RandomNodeSplit(num_val=1000, num_test=1500),\n",
    "    T.TargetIndegree(),])# this is what is called a transform, basically it is a way of \"transforming\" our data. In this case it \n",
    "                        # randomizes the nodes and creates a training/validation/testing split\n",
    "dataset = Planetoid(root=\"tmp/Cora\", name=\"Cora\",transform=transform)# this grabs the cora dataset, already well ordered so we do not need a \n",
    "# utils module\n",
    "data = dataset[0]# this just grabs the first graph from the dataset(some datasets train on several graphs, ours just uses one)\n",
    "model = GCN(dataset.num_node_features,16,dataset.num_classes).to(\"cuda:0\")# this initializes our model and puts it on the GPU(change if you do not have a gpu)\n",
    "data.to(\"cuda:0\")# puts our data on the GPU as well\n",
    "optimizer = torch.optim.Adam(params=model.parameters(),lr = .01, weight_decay=5e-3)# sets up our optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds,mask):# obtains the accuracy of the model\n",
    "    correct = (preds[mask] == data.y[mask]).sum()\n",
    "    acc = int(correct)/int(mask.sum())\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 training_loss: 0.1681639403104782 training_acc: 0.9855769230769231 val_loss: 0.6935117244720459 val_acc: 0.786\n",
      "epoch: 5 training_loss: 0.15806561708450317 training_acc: 0.9807692307692307 val_loss: 0.6871837377548218 val_acc: 0.772\n",
      "epoch: 10 training_loss: 0.1618833839893341 training_acc: 0.9759615384615384 val_loss: 0.7041781544685364 val_acc: 0.782\n",
      "epoch: 15 training_loss: 0.13891062140464783 training_acc: 0.9903846153846154 val_loss: 0.6866135597229004 val_acc: 0.785\n",
      "epoch: 20 training_loss: 0.16179326176643372 training_acc: 0.9951923076923077 val_loss: 0.6573135852813721 val_acc: 0.791\n",
      "epoch: 25 training_loss: 0.13362950086593628 training_acc: 0.9807692307692307 val_loss: 0.6789388656616211 val_acc: 0.792\n",
      "epoch: 30 training_loss: 0.15979844331741333 training_acc: 0.9855769230769231 val_loss: 0.6920156478881836 val_acc: 0.794\n",
      "epoch: 35 training_loss: 0.14456354081630707 training_acc: 0.9903846153846154 val_loss: 0.6908521056175232 val_acc: 0.782\n",
      "epoch: 40 training_loss: 0.15017399191856384 training_acc: 0.9807692307692307 val_loss: 0.6995054483413696 val_acc: 0.782\n",
      "epoch: 45 training_loss: 0.17453402280807495 training_acc: 0.9711538461538461 val_loss: 0.6912053823471069 val_acc: 0.787\n",
      "epoch: 50 training_loss: 0.16218625009059906 training_acc: 0.9711538461538461 val_loss: 0.6680435538291931 val_acc: 0.796\n",
      "epoch: 55 training_loss: 0.13358792662620544 training_acc: 0.9903846153846154 val_loss: 0.6459622979164124 val_acc: 0.801\n",
      "epoch: 60 training_loss: 0.1575494408607483 training_acc: 0.9855769230769231 val_loss: 0.7009108662605286 val_acc: 0.787\n",
      "epoch: 65 training_loss: 0.14089156687259674 training_acc: 0.9951923076923077 val_loss: 0.6712456941604614 val_acc: 0.791\n",
      "epoch: 70 training_loss: 0.14475375413894653 training_acc: 0.9807692307692307 val_loss: 0.6830351948738098 val_acc: 0.795\n",
      "epoch: 75 training_loss: 0.14868053793907166 training_acc: 0.9855769230769231 val_loss: 0.6501244902610779 val_acc: 0.815\n",
      "epoch: 80 training_loss: 0.14071892201900482 training_acc: 0.9903846153846154 val_loss: 0.6688368320465088 val_acc: 0.794\n",
      "epoch: 85 training_loss: 0.1557893604040146 training_acc: 0.9855769230769231 val_loss: 0.6627652645111084 val_acc: 0.795\n",
      "epoch: 90 training_loss: 0.14479519426822662 training_acc: 0.9903846153846154 val_loss: 0.6747444868087769 val_acc: 0.796\n",
      "epoch: 95 training_loss: 0.1650257110595703 training_acc: 0.9663461538461539 val_loss: 0.6809121370315552 val_acc: 0.783\n",
      "epoch: 100 training_loss: 0.1539452224969864 training_acc: 0.9903846153846154 val_loss: 0.7005966901779175 val_acc: 0.783\n",
      "epoch: 105 training_loss: 0.14923140406608582 training_acc: 0.9903846153846154 val_loss: 0.6679930090904236 val_acc: 0.785\n",
      "epoch: 110 training_loss: 0.15354543924331665 training_acc: 0.9855769230769231 val_loss: 0.7137076258659363 val_acc: 0.77\n",
      "epoch: 115 training_loss: 0.14117935299873352 training_acc: 0.9903846153846154 val_loss: 0.6793891787528992 val_acc: 0.789\n",
      "epoch: 120 training_loss: 0.15532925724983215 training_acc: 0.9855769230769231 val_loss: 0.6840073466300964 val_acc: 0.783\n",
      "epoch: 125 training_loss: 0.15272478759288788 training_acc: 0.9807692307692307 val_loss: 0.6547684669494629 val_acc: 0.794\n",
      "epoch: 130 training_loss: 0.1390513777732849 training_acc: 0.9951923076923077 val_loss: 0.6676009297370911 val_acc: 0.793\n",
      "epoch: 135 training_loss: 0.1374889612197876 training_acc: 0.9903846153846154 val_loss: 0.6583476066589355 val_acc: 0.804\n",
      "epoch: 140 training_loss: 0.15874817967414856 training_acc: 0.9903846153846154 val_loss: 0.7048813104629517 val_acc: 0.781\n",
      "epoch: 145 training_loss: 0.14464329183101654 training_acc: 0.9759615384615384 val_loss: 0.66742342710495 val_acc: 0.808\n",
      "epoch: 150 training_loss: 0.1386331170797348 training_acc: 0.9855769230769231 val_loss: 0.6930668354034424 val_acc: 0.786\n",
      "epoch: 155 training_loss: 0.15536123514175415 training_acc: 0.9903846153846154 val_loss: 0.6992133855819702 val_acc: 0.792\n",
      "epoch: 160 training_loss: 0.1416189968585968 training_acc: 0.9807692307692307 val_loss: 0.7031627893447876 val_acc: 0.786\n",
      "epoch: 165 training_loss: 0.16363312304019928 training_acc: 0.9759615384615384 val_loss: 0.6810053586959839 val_acc: 0.791\n",
      "epoch: 170 training_loss: 0.16356395184993744 training_acc: 0.9903846153846154 val_loss: 0.6993815302848816 val_acc: 0.789\n",
      "epoch: 175 training_loss: 0.14794328808784485 training_acc: 0.9903846153846154 val_loss: 0.6867456436157227 val_acc: 0.792\n",
      "epoch: 180 training_loss: 0.14619380235671997 training_acc: 0.9903846153846154 val_loss: 0.6961743235588074 val_acc: 0.78\n",
      "epoch: 185 training_loss: 0.14970368146896362 training_acc: 0.9903846153846154 val_loss: 0.6978273987770081 val_acc: 0.797\n",
      "epoch: 190 training_loss: 0.12868709862232208 training_acc: 0.9855769230769231 val_loss: 0.6602802872657776 val_acc: 0.805\n",
      "epoch: 195 training_loss: 0.15778133273124695 training_acc: 0.9759615384615384 val_loss: 0.7043335437774658 val_acc: 0.785\n",
      "0.832\n"
     ]
    }
   ],
   "source": [
    "model.train()# tells our model we are about to train\n",
    "for epoch in range(200):# runs through all the data 200 times\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "\n",
    "    train_loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    train_acc = accuracy(out.max(1)[1], data.train_mask)\n",
    "\n",
    "    val_loss = F.nll_loss(out[data.val_mask], data.y[data.val_mask])\n",
    "    val_acc = accuracy(out.max(1)[1], data.val_mask)\n",
    "\n",
    "    if(epoch %5 == 0):\n",
    "        print(\"epoch: {} training_loss: {} training_acc: {} val_loss: {} val_acc: {}\".format(epoch,train_loss,train_acc,val_loss, val_acc))\n",
    "        \n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "model.eval()\n",
    "preds = model(data).max(1)[1]\n",
    "acc = accuracy(preds,data.test_mask)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9a1173fe2fc6b1e069ff4bb519a0fac3f1641eed16cc368a7915ccd160d7c0f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mainenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
